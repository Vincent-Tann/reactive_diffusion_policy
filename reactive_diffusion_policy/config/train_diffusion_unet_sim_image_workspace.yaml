defaults:
  - _self_
  - task: sim_square_image_dp_12fps

name: diffusion_unet_sim_image
_target_: reactive_diffusion_policy.workspace.diffusion_unet_image_workspace.DiffusionUnetImageWorkspace

seed: 42
device: cuda:0

horizon: 32
n_obs_steps: 2
n_latency_steps: 0
n_action_steps: ${eval:'${horizon}-${n_obs_steps}+1-${n_latency_steps}'}
dataset_obs_steps: ${n_obs_steps}

model:
  _target_: reactive_diffusion_policy.policy.diffusion_unet_image_policy.DiffusionUnetImagePolicy

  horizon: ${horizon}
  n_action_steps: ${n_action_steps}
  n_obs_steps: ${n_obs_steps}
  noise_scheduler:
    _target_: diffusers.schedulers.scheduling_ddpm.DDPMScheduler
    num_train_timesteps: 100
    beta_start: 0.0001
    beta_end: 0.02
    beta_schedule: squaredcos_cap_v2
    variance_type: fixed_small
    clip_sample: True
    prediction_type: epsilon
  
  # diffusion params
  num_inference_steps: 100
  
  # unet params
  unet_in_channels: 10
  unet_out_channels: 4
  unet_down_block_out_channels: [32, 64, 128, 256]
  unet_down_block_kernel_sizes: [5, 5, 5, 5]
  unet_down_block_strides: [2, 2, 2, 2]
  unet_up_block_kernel_sizes: [5, 5, 5, 5]
  unet_up_block_strides: [2, 2, 2, 2]
  unet_padding_mode: replicate
  
  # image encoder params
  image_encoder:
    _target_: reactive_diffusion_policy.model.vision.resnet18_encoder.ResNet18Encoder
    input_channels: 3
    pretrained: True
    pool_type: avg
    add_feature_pos: True
    global_pool: True
    feature_keys: ['layer4']
    output_dim: 512
  
  # low-dim encoder params
  obs_encoder:
    _target_: reactive_diffusion_policy.model.common.mlp.MLP
    input_dim: ${eval:'${n_obs_steps}*8'}  # Adjust based on your low-dim observation size
    output_dim: 128
    hidden_dims: [256, 256]
    activation: relu
    norm_layer: null
  
  # action decoder params
  action_decoder:
    _target_: reactive_diffusion_policy.model.common.mlp.MLP
    input_dim: 256
    output_dim: ${eval:'${n_action_steps}*7'}  # Adjust based on your action size
    hidden_dims: [256, 256]
    activation: relu
    norm_layer: null

training:
  # training params
  epochs: 2000
  batch_size: 64
  num_workers: 4
  shuffle: True
  lr: 1.0e-4
  weight_decay: 0.0
  betas: [0.9, 0.95]
  grad_norm_clip: 1.0
  use_ema: True
  ema_decay: 0.995
  warmup_steps: 500
  
  # checkpoint params
  checkpoint_interval: 100
  validation_interval: 100
  num_checkpoint_limit: 10
  
  # loss params
  loss_weight_act: 1.0
  
  # validation params
  val_batch_size: 64
  
  # logging params
  log_interval: 10
  
  # optimizer params
  optimizer:
    _target_: torch.optim.AdamW
    lr: ${training.lr}
    weight_decay: ${training.weight_decay}
    betas: ${training.betas}
  
  # scheduler params
  scheduler:
    _target_: reactive_diffusion_policy.model.common.lr_scheduler.CosineAnnealingWarmupRestarts
    first_cycle_steps: ${training.epochs}
    cycle_mult: 1.0
    max_lr: ${training.lr}
    min_lr: 1.0e-6
    warmup_steps: ${training.warmup_steps}
    gamma: 1.0 